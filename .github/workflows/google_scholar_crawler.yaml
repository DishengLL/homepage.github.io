name: Get Citation Data

on:
  page_build:
  schedule:
    - cron: '0 8 * * *'   #
  workflow_dispatch:       # 

jobs:
  build:
    runs-on: ubuntu-latest
    timeout-minutes: 15   

    steps:
    - name: Checkout repo
      uses: actions/checkout@v4

    - name: Set up Python 3.12
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'

    - name: Install dependencies
      working-directory: google_scholar_crawler
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Run crawler (with timeout)
      working-directory: google_scholar_crawler
      env:
        GOOGLE_SCHOLAR_ID: ${{ secrets.GOOGLE_SCHOLAR_ID }}
      run: |
        set -e
        echo ">>> Start crawler"
        # 最多跑 5 分钟，超时直接失败，避免“卡死看不到原因”
        timeout 300s python -u main.py
        echo ">>> Crawler finished"

    - name: Commit and push results
      working-directory: google_scholar_crawler/results
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        GITHUB_ACTOR: ${{ github.actor }}
        GITHUB_REPOSITORY: ${{ github.repository }}
      run: |
        set -e
        git init
        git config user.name "${GITHUB_ACTOR}"
        git config user.email "${GITHUB_ACTOR}@users.noreply.github.com"

        if ls *.json >/dev/null 2>&1; then
          git add *.json
          git commit -m "Updated Citation Data" || echo "No changes to commit"
          git branch -M google-scholar-stats
          git remote add origin "https://${GITHUB_ACTOR}:${GITHUB_TOKEN}@github.com/${GITHUB_REPOSITORY}.git"
          git push origin google-scholar-stats --force
        else
          echo "No JSON files to commit"
        fi